# FinRAG System Architecture

This document provides structural overviews of the codebase organization and data flow patterns. Later in the code files, please do pay attention to multiple `_contract.py` files which have excellent architectural flow diagrams, Data-Entity flows, Data-Responsibility understanding and more. 

Example: `ModelPipeline\finrag_ml_tg1\rag_modules_src\synthesis_pipeline\models.py`, `ModelPipeline\finrag_ml_tg1\rag_modules_src\rag_pipeline\models.py` etc.

## Project Directory Structure
### Parent Directory Structure Overview:

```
ðŸ“¦ finrag_ml_tg1/
 â”£ ðŸ“‚ __pycache__/                              # Python bytecode cache (auto-generated)
 â”£ ðŸ“‚ .aws_config/                              # AWS service configuration files
 â”£ ðŸ“‚ .aws_secrets/                             # AWS credentials and sensitive keys (gitignored)
 â”£ ðŸ“‚ data_cache/                               # Local data storage for intermediate processing results
 â”£ ðŸ“‚ environments/                             # Conda/Python environment specifications and dependencies
 â”£ ðŸ“‚ loaders/                                  # Data loading utilities and ETL ingestion modules
 â”£ ðŸ“‚ platform_core_notebooks/                  # Core development notebooks for data lifecycle management
 â”£ ðŸ“‚ rag_modules_src/                          # Production RAG pipeline components (query-time execution)
 â”£ ðŸ“‚ venv_ml_rag/                              # Python virtual environment (local development)
 â”£ ðŸ“œ __init__.py                               # Python package initialization
 â”£ ðŸ“œ .python-version                           # Python version specification for project
 â”£ ðŸ“œ ML_Modelling_README.md                    # Main project documentation and architecture overview
 â”— ðŸ“œ S3Vect_QueryCost.md                       # S3 Vector store cost analysis and projections

ðŸ“¦ finrag_ml_tg2/                               # [Secondary project workspace or experimental branch]

ðŸ“¦ serving/                               		# serving - backend FastAPI + pydantic, streamlit etc.
 â”£ ðŸ“‚ frontend/                                # Frontend Streamlit application for user interaction
 â”£ ðŸ“‚ backend/                                 # Backend FastAPI service for model inference and API endpoints


ðŸ“œ .dvcignore                                   # DVC ignore patterns for data version control
ðŸ“œ .gitignore                                   # Git ignore patterns for version control
```

### Embedding-Infra and Spines Overview:

```

ðŸ“¦ platform_core_notebooks/
 â”£ ðŸ“œ 01_Stage2_EmbeddingGen.ipynb              # Stage 2 meta table creation + embedding generation pipeline
 â”£ ðŸ“œ 02_EmbeddingAnalytics.ipynb               # Vector-metadata parity, staleness checks, integration audits
 â”£ ðŸ“œ 03_S3Vector_TableProvisioning.ipynb       # S3 Vector store schema setup and initialization
 â”£ ðŸ“œ 04_S3Vector_BulkIngestion.ipynb           # Mass vector insertion pipeline (200K+ vectors)
	
	# these are 'dormant', infra_setup_notebooks which are performed in low frequency, expected: twice a year.

 ðŸ“¦ validation_notebooks/
 â”£ ðŸ“œ 05_GoldP1P2_TestSuite.ipynb               # Validation framework, anchor design, Gold P1/P2 methodology
 â”£ ðŸ“œ 06_GoldP3_HeuristicEng_Curation.ipynb     # Query taxonomy, warehouse design, NLP-heuristic curation for Gold P3
 â”£ ðŸ“œ 07_S3_CostProjections.ipynb               # Query cost modeling and operational expense analysis
 â”£ ðŸ“œ 08_RAGArch_DesignNotes.ipynb              # RAG architecture decisions, technical rationale, design patterns
 â”£ ðŸ“œ 09_RAG_Comp_ITests_01.ipynb               # Component-level tests for entity adapter and early integration
	
	# can be deal with as 'iterative development artifacts', for dev tests.
```

### RAG Modules Source Code Overview:
```
ðŸ“¦ rag_modules_src/
 â”£ ðŸ“‚ 01_Isolation_Test_NBS/                    # Isolated unit tests and component validation notebooks
 â”£ ðŸ“‚ constants/                                # Project-wide constants, configurations, and static definitions
 â”£ ðŸ“‚ entity_adapter/                           # Entity extraction and structured KPI data transformation logic
 â”£ ðŸ“‚ exports/                                  # Output formatting, result serialization, and data export utilities
 â”£ ðŸ“‚ metric_pipeline/                          # KPI extraction pipeline for structured financial metrics
 â”£ ðŸ“‚ prompts/                                  # LLM prompt templates and instruction engineering modules
 â”£ ðŸ“‚ rag_pipeline/                             # Core RAG orchestration: retrieval, reranking, context assembly
 â”£ ðŸ“‚ synthesis_pipeline/                       # LLM response generation and answer synthesis logic
 â”£ ðŸ“‚ test_outputs/                             # Test results, validation artifacts, and debugging outputs
 â”£ ðŸ“‚ utilities/                                # Shared helper functions, logging, error handling, common tools
 â”— ðŸ“œ __init__.py                               # Python package initialization for rag_modules_src
```

### - Important Testing and Isolation Notebooks:
```
ðŸ“¦ rag_modules_src
 â”£ ðŸ“‚ 01_Isolation_Test_NBS/                    # Component-level isolation tests for RAG pipeline stages
 â”ƒ â”£ ðŸ“œ 00_ITest_SupplyLine1.ipynb             # Tests initial data ingestion and entity loading mechanisms
 â”ƒ â”£ ðŸ“œ 01_ITest_Supply12_EntityEmbed.ipynb    # Validates entity extraction and embedding generation pipeline
 â”ƒ â”£ ðŸ“œ 02_ITest_RetrievalSpine_Steps4to7.ipynb # Tests core retrieval logic: query embedding â†’ vector search â†’ scoring
 â”ƒ â”£ ðŸ“œ 03_ITest_Variant_Retrieve.ipynb        # Validates alternative retrieval strategies and performance variants
 â”ƒ â”£ ðŸ“œ 05_ITest_ExpanderDedupe.ipynb          # Tests context expansion and duplicate sentence deduplication logic
 â”ƒ â”£ ðŸ“œ 06_ITest_RetrievalSpine_Steps8to10.ipynb # Validates reranking, context assembly, and final retrieval output
 â”ƒ â”— ðŸ“œ 07_ITest_Assembled_Serve_1_2.ipynb     # End-to-end test: retrieval pipeline â†’ formatted context for LLM serving
 â”ƒ
 â”£ ðŸ“‚ 02_LLMEval_Notebooks/                     # LLM inference evaluation and response quality validation
 â”ƒ â”£ ðŸ“œ 08_ITest_Start_To_LLM_Serve.ipynb      # Full pipeline test: query â†’ retrieval â†’ LLM generation â†’ response
 â”ƒ â”£ ðŸ“œ 09_ITest_LLM_Serves_P3.ipynb           # Extended LLM serving tests with production prompt templates
 â”ƒ â”£ ðŸ“œ 10_ITest_LLM_Log_Analytics.ipynb       # LLM request/response logging, token usage, and cost analysis
 â”ƒ â”£ ðŸ“œ 11_ITest_AnsScoring.ipynb              # Answer quality evaluation: ROUGE-L, BERTScore, factual accuracy metrics
 â”ƒ â”— ðŸ“œ 12_ITest_Func_BussToMetric.ipynb       # Tests structured KPI extraction from business-level queries
 â”ƒ
 â”— ðŸ“‚ 03_LambdaRefactor_Tests/                  # AWS Lambda deployment architecture and abstraction layer tests
   â”£ ðŸ“œ 16_DataLoaderFactory_Tests.ipynb        # Tests DataLoader factory pattern for environment-agnostic initialization
   â”— ðŸ“œ 17_MockLambda_S3Loader_T1.ipynb         # Validates Lambda-compatible S3 data loading with mock AWS environment
```



### Summary of Entity-Chaining and Flows:
**(Semantic Search + Context Assembly)**
```
User Raw Query:
	â†’ EntityAdapter / Extraction 
	â†’ QueryEmbedderV2 
	â†’ MetadataFilterBuilder 
	â†’ VariantPipeline (LLM rephrasings + re-embeds)
	â†’ S3VectorsRetriever (filtered + global regimes, plus variants)
	â†’ Post Retrieve - dedupe + per-source stratified top percentile selection.
	â†’ SentenceExpander (edge/window expansion + d2 overlap-dedup)
	â†’ Core Hit + Non-Core Neighbour Provenance Tracking, Provenance Aggregation
	â†’ ContextAssembler (sort + headered, chronological + logical grouping - based assembly)
	â†’ [ Returnable ] 
```

**Metric Extraction Pipeline**
```
User Raw Query:
	â†’ EntityAdapter / Extraction 
	â†’ Metric processor - extractor (Extended Vishak's and made V2 fix versions/reused logics.)
	â†’ Entity-Meta, Header Enhanced Util (analytical data formatting/assembly)
	â†’ [ SupplyLine / Wiring - Returnable. ]
```

**End-to-End Orchestration (Query â†’ Answer Generation)**
```
User Raw Query
	â†’ RAGOrchestrator (initialize pipeline components)
	â†’ RAG Retrieval Pipeline (semantic context assembly)
	â†’ Metric Extraction Pipeline (structured KPI extraction)
	â†’ Context Merge (combine narrative + structured data)
	â†’ PromptLoader (load template + inject assembled context)
	â†’ BedrockClient (LLM inference with prompt)
	â†’ SynthesisPipeline (format response + add citations)
	â†’ [ Final Answer Returned to User ]
```


### Data Dependencies:
- Stage 2 Meta Table: finrag_fact_sentences_meta_embeds.parquet (~73MB)
- Stage 3 S3 Vectors Table: finrag_s3vectors_cohere_1024d.parquet (~360MB)
- Dimension Tables: Companies, Sections (small)
- Metric Data: downloaded_data.json (KPI lookup)

## Next Steps
â†’ See [IMPLEMENTATION_GUIDE.md](IMPLEMENTATION_GUIDE.md) for detailed technical implementation across 10 major development phases.


### Dual Environment Approach: (Potential)

- ENVIRONMENT 1: venv_ml_rag (FULL - Keep for Analytics)
```
  Location: finrag_ml_tg1/venv_ml_rag/
  Uses: environments/requirements.txt
  Size: ~2-3GB
  Purpose: Notebooks, eval metrics, torch, transformers
  When: Running platform_core_notebooks/, validation work
```

- ENVIRONMENT 2: venv_serving (MINIMAL - New for Deployment)
```
  Location: finrag_ml_tg1/venv_serving/  (NEW!)
  Uses: environments/requirements-sevalla.txt
  Size: ~500MB
  Purpose: Backend + Frontend serving ONLY
  When: Running start_finrag, testing deployment, Sevalla cloud
```


### Architecture Author:
Author: Joel Markapudi. ( markapudi.j@northeastern.edu, mjsushanth@gmail.com )
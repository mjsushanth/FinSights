{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18c0a8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Joel Markapudi. \n",
    "- 2-Nov Start\n",
    "- Model dev work, experiments, brainstorming on design, etc.\n",
    "- Will work here first, move onto clean py files later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93529b",
   "metadata": {},
   "source": [
    "### Why so much cost tracking and token-wise API tracking analysis? Costs.\n",
    "\n",
    "- console itself runs inside an AWS-managed web app -> browser fetches a pre-signed HTTPS URL generated by the console service -> in-region access or intra-AWS traffic, which is often free. but.\n",
    "- personal downloads also route through CloudFront or their internal edge acceleration, which may absorb small transfer fees.\n",
    "- Batch ETL pulling hundreds of GB per day from S3 to local, Repeated egress from multiple regions - Awareness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3e304",
   "metadata": {},
   "source": [
    "### Potential Structure for S3.\n",
    "```\n",
    "├── DATA_MERGE_ASSETS/                 # existing structure\n",
    "│   ├── FINRAG_FACT_SENTENCES/\n",
    "│   └── FINRAG_FACT_METRICS/\n",
    "│\n",
    "└── ML_EMBED_ASSETS/                        \n",
    "    ├── EMBED_META_FACT/\n",
    "    │   └── finrag_fact_sentences_final.parquet\n",
    "    │\n",
    "    └── EMBED_VECTORS/\n",
    "        ├── cohere_v3_768d/\n",
    "        │   ├── finrag_embeddings_cohere_v3.parquet\n",
    "        │   ├── metadata.json\n",
    "        │   └── validation_report.json\n",
    "        │\n",
    "        └── titan_v2_1024d/            # Future\n",
    "            └── ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca70a78",
   "metadata": {},
   "source": [
    "### Tests 1 - 2, check for boto3.client and then check for model access through AWS org account credentials. Works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c16013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Bedrock client created successfully\n",
      "  Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# Zero-cost test (no API call)\n",
    "import boto3\n",
    "\n",
    "try:\n",
    "    bedrock = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "    print(\"✓ Bedrock client created successfully\")\n",
    "    print(f\"  Region: {bedrock.meta.region_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6984d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "✓ Config loaded\n",
      "  Bucket: sentence-data-ingestion\n",
      "  Region: us-east-1\n",
      "  Model: cohere.embed-v4:0\n",
      "✓ Bedrock API works!\n",
      "  Model: cohere.embed-v4:0\n",
      "  Dimensions: 1024\n",
      "  First 5 values: [0.048339844, 0.040039062, -0.020874023, -0.012573242, -0.044921875]\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add loaders to path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from ml_config_loader import MLConfig\n",
    "\n",
    "# Initialize config (loads AWS credentials automatically)\n",
    "config = MLConfig()\n",
    "\n",
    "print(\"✓ Config loaded\")\n",
    "print(f\"  Bucket: {config.bucket}\")\n",
    "print(f\"  Region: {config.region}\")\n",
    "print(f\"  Model: {config.bedrock_model_id}\")\n",
    "\n",
    "\n",
    "\n",
    "# Cell 2: Test Bedrock API\n",
    "import json\n",
    "\n",
    "# Get Bedrock client (uses config credentials)\n",
    "bedrock = config.get_bedrock_client()\n",
    "\n",
    "# Test embedding with v4\n",
    "body = json.dumps({\n",
    "    \"texts\": [\"Revenue increased significantly.\"],\n",
    "    \"input_type\": config.bedrock_input_type,\n",
    "    \"embedding_types\": [\"float\"],\n",
    "    \"output_dimension\": config.bedrock_dimensions\n",
    "})\n",
    "\n",
    "response = bedrock.invoke_model(\n",
    "    body=body,\n",
    "    modelId=config.bedrock_model_id,\n",
    "    accept='*/*',\n",
    "    contentType='application/json'\n",
    ")\n",
    "\n",
    "result = json.loads(response['body'].read())\n",
    "embeddings = result['embeddings']['float']\n",
    "\n",
    "print(f\"✓ Bedrock API works!\")\n",
    "print(f\"  Model: {config.bedrock_model_id}\")\n",
    "print(f\"  Dimensions: {len(embeddings[0])}\")\n",
    "print(f\"  First 5 values: {embeddings[0][:5]}\")\n",
    "# print(f\"  Cost: ~$0.0000005\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7a8c0",
   "metadata": {},
   "source": [
    "### Prep work: 01: Load S3 fact sentences, modify and create new columns, save back to S3 - finrag_fact_sentences_meta_embeds.parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443e95a",
   "metadata": {},
   "source": [
    "### Analysis and patterns.\n",
    "- Apple 2016 ITEM_1:\n",
    "  first: 0000320193_10-K_2016_section_1_0\n",
    "  last:  0000320193_10-K_2016_section_1_99\n",
    "- Pattern: `{CIK}_{filing}_{year}_section_{section_ID}_{sequence}`\n",
    "- We'll create the shifts of plus one and minus one for the sentence ID to perform a concept of previous and next sentence ID. But this is a rough scheme or an idea. Later we may not actually use this thoroughly because we cannot really depend on this particular element. element if the clustered key or unique key from various sources is not following the exact same pattern.\n",
    "- Local file downloaded at: ModelPipeline\\finrag_ml_tg1\\data_cache\\fact_sentences\\finrag_fact_sentences.parquet\n",
    "\n",
    "- \"Revenue increased 15% to $274.5 billion.\" Average across millions of English sentences: 1 word ≈ 1.33 tokens\n",
    "  ``` \n",
    "  - Word count: 6 words\n",
    "  - Token count (actual): 9 tokens\n",
    "    - ['Revenue', 'increased', '15', '%', 'to', '$', '274', '.', '5', 'billion', '.']\n",
    "  - Approximation: 6 × 1.33 = 8 tokens\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be2985b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\fact_sentences\\finrag_fact_sentences.parquet\n",
      "✓ Loaded: 469,252 rows\n",
      "\n",
      "======================================================================\n",
      "SENTENCEID PATTERN VALIDATION\n",
      "======================================================================\n",
      "\n",
      "Total rows: 469,252\n",
      "\n",
      "[Full Pattern: CIK_filing_year_section_sectionID_sequence]\n",
      "  Valid: 469,252 (100.00%)\n",
      "  Invalid: 0 (0.00%)\n",
      "\n",
      "[Numeric Suffix Only: ends with number]\n",
      "  Valid: 469,252 (100.00%)\n",
      "  Invalid: 0 (0.00%)\n",
      "\n",
      "✓ Examples of VALID sentenceIDs (parsed):\n",
      "  0000034088_10-K_2006_section_10_1\n",
      "    → CIK: 0000034088, Year: 2006, Section: 10, Seq: 1\n",
      "  0000034088_10-K_2006_section_11_2\n",
      "    → CIK: 0000034088, Year: 2006, Section: 11, Seq: 2\n",
      "  0000034088_10-K_2006_section_12_10\n",
      "    → CIK: 0000034088, Year: 2006, Section: 12, Seq: 10\n",
      "  0000034088_10-K_2006_section_12_11\n",
      "    → CIK: 0000034088, Year: 2006, Section: 12, Seq: 11\n",
      "  0000034088_10-K_2006_section_12_13\n",
      "    → CIK: 0000034088, Year: 2006, Section: 12, Seq: 13\n",
      "\n",
      "[Component Validation]\n",
      "  CIK extracted: 469,252 rows\n",
      "  Filing extracted: 469,252 rows\n",
      "  Year extracted: 469,252 rows\n",
      "  Section extracted: 469,252 rows\n",
      "  Sequence extracted: 469,252 rows\n",
      "\n",
      "======================================================================\n",
      "✅ RECOMMENDATION: Pattern highly reliable (≥95%)\n",
      "   → Safe to use shift() for prev/next_sentenceID\n",
      "   → Sequence numbers are trustworthy for ordering\n",
      "======================================================================\n",
      "Latest year: 2025\n",
      "Sample sentenceIDs:\n",
      "  0000104169_10-K_2025_section_10_0\n",
      "  0000104169_10-K_2025_section_10_1\n",
      "  0000104169_10-K_2025_section_10_10\n",
      "  0000104169_10-K_2025_section_10_11\n",
      "  0000104169_10-K_2025_section_10_2\n",
      "  0000104169_10-K_2025_section_10_3\n",
      "  0000104169_10-K_2025_section_10_4\n",
      "  0000104169_10-K_2025_section_10_5\n",
      "  0000104169_10-K_2025_section_10_6\n",
      "  0000104169_10-K_2025_section_10_7\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Load from local cache\n",
    "cache_file = Path.cwd().parent / 'data_cache' / 'fact_sentences' / 'finrag_fact_sentences.parquet'\n",
    "print(f\"Loading: {cache_file}\")\n",
    "\n",
    "df = pl.read_parquet(cache_file)\n",
    "print(f\"✓ Loaded: {len(df):,} rows\\n\")\n",
    "\n",
    "# Define expected pattern\n",
    "# Format: {CIK}_{filing}_{year}_section_{section_ID}_{sequence}\n",
    "# Example: 0000320193_10-K_2016_section_1_42\n",
    "pattern = r'^(\\d{10})_(10-[KQ]|8-K)_(\\d{4})_section_(\\w+)_(\\d+)$'\n",
    "\n",
    "# Validate pattern\n",
    "df_validated = df.with_columns([\n",
    "    # Check if matches full pattern\n",
    "    pl.col('sentenceID').str.contains(pattern).alias('_matches_full_pattern'),\n",
    "    \n",
    "    # Extract components\n",
    "    pl.col('sentenceID').str.extract(pattern, 1).alias('_cik_part'),\n",
    "    pl.col('sentenceID').str.extract(pattern, 2).alias('_filing_part'),\n",
    "    pl.col('sentenceID').str.extract(pattern, 3).alias('_year_part'),\n",
    "    pl.col('sentenceID').str.extract(pattern, 4).alias('_section_part'),\n",
    "    pl.col('sentenceID').str.extract(pattern, 5).alias('_sequence_part'),\n",
    "    \n",
    "    # Check numeric sequence specifically\n",
    "    pl.col('sentenceID').str.split('_').list.last()\n",
    "        .cast(pl.Int32, strict=False)\n",
    "        .is_not_null()\n",
    "        .alias('_has_numeric_suffix')\n",
    "])\n",
    "\n",
    "# Calculate statistics\n",
    "total_rows = len(df_validated)\n",
    "full_pattern_valid = df_validated.filter(pl.col('_matches_full_pattern')).shape[0]\n",
    "numeric_suffix_valid = df_validated.filter(pl.col('_has_numeric_suffix')).shape[0]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SENTENCEID PATTERN VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal rows: {total_rows:,}\")\n",
    "print(f\"\\n[Full Pattern: CIK_filing_year_section_sectionID_sequence]\")\n",
    "print(f\"  Valid: {full_pattern_valid:,} ({full_pattern_valid/total_rows*100:.2f}%)\")\n",
    "print(f\"  Invalid: {total_rows - full_pattern_valid:,} ({(1-full_pattern_valid/total_rows)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n[Numeric Suffix Only: ends with number]\")\n",
    "print(f\"  Valid: {numeric_suffix_valid:,} ({numeric_suffix_valid/total_rows*100:.2f}%)\")\n",
    "print(f\"  Invalid: {total_rows - numeric_suffix_valid:,} ({(1-numeric_suffix_valid/total_rows)*100:.2f}%)\")\n",
    "\n",
    "# Show invalid examples\n",
    "invalid_full = df_validated.filter(~pl.col('_matches_full_pattern'))\n",
    "if len(invalid_full) > 0:\n",
    "    print(f\"\\nExamples of INVALID sentenceIDs (full pattern):\")\n",
    "    for row in invalid_full.select('sentenceID').head(10).iter_rows():\n",
    "        print(f\"  - {row[0]}\")\n",
    "\n",
    "# Show valid examples with parsed components\n",
    "valid_samples = df_validated.filter(pl.col('_matches_full_pattern')).head(5)\n",
    "print(f\"\\n✓ Examples of VALID sentenceIDs (parsed):\")\n",
    "for row in valid_samples.select(['sentenceID', '_cik_part', '_year_part', '_section_part', '_sequence_part']).iter_rows():\n",
    "    print(f\"  {row[0]}\")\n",
    "    print(f\"    → CIK: {row[1]}, Year: {row[2]}, Section: {row[3]}, Seq: {row[4]}\")\n",
    "\n",
    "# Component-level validation\n",
    "print(f\"\\n[Component Validation]\")\n",
    "print(f\"  CIK extracted: {df_validated.filter(pl.col('_cik_part').is_not_null()).shape[0]:,} rows\")\n",
    "print(f\"  Filing extracted: {df_validated.filter(pl.col('_filing_part').is_not_null()).shape[0]:,} rows\")\n",
    "print(f\"  Year extracted: {df_validated.filter(pl.col('_year_part').is_not_null()).shape[0]:,} rows\")\n",
    "print(f\"  Section extracted: {df_validated.filter(pl.col('_section_part').is_not_null()).shape[0]:,} rows\")\n",
    "print(f\"  Sequence extracted: {df_validated.filter(pl.col('_sequence_part').is_not_null()).shape[0]:,} rows\")\n",
    "\n",
    "# Final recommendation\n",
    "print(f\"\\n{'='*70}\")\n",
    "if full_pattern_valid / total_rows >= 0.95:\n",
    "    print(\"✅ RECOMMENDATION: Pattern highly reliable (≥95%)\")\n",
    "    print(\"   → Safe to use shift() for prev/next_sentenceID\")\n",
    "    print(\"   → Sequence numbers are trustworthy for ordering\")\n",
    "elif numeric_suffix_valid / total_rows >= 0.95:\n",
    "    print(\"RECOMMENDATION: Full pattern has issues, but numeric suffix reliable\")\n",
    "    print(\"   → Can use shift() but validate sorting carefully\")\n",
    "else:\n",
    "    print(\"❌ RECOMMENDATION: Pattern unreliable (<95%)\")\n",
    "    print(\"   → Skip prev/next_sentenceID columns\")\n",
    "    print(\"   → Use runtime neighbor lookups instead\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Show sentenceIDs from most recent year\n",
    "latest_year = df['report_year'].max()\n",
    "sample = df.filter(pl.col('report_year') == latest_year).select('sentenceID').head(10)\n",
    "\n",
    "print(f\"Latest year: {latest_year}\\nSample sentenceIDs:\")\n",
    "for sid in sample['sentenceID']:\n",
    "    print(f\"  {sid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972179bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "DATA PREPARATION PIPELINE\n",
      "======================================================================\n",
      "Model: cohere.embed-v4:0 (1024d)\n",
      "\n",
      "[Stage 1 Table - Downloading from S3]\n",
      "  Source: s3://sentence-data-ingestion/DATA_MERGE_ASSETS/FINRAG_FACT_SENTENCES/finrag_fact_sentences.parquet\n",
      "  Size: 23.1 MB\n",
      "  Downloaded: 469,252 rows (Cost: $0.0020 egress)\n",
      "  ✓ Cached to: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\stage1_facts\\finrag_fact_sentences.parquet\n",
      "\n",
      "[Stage 2 Meta Table - Downloading from S3]\n",
      "  Source: s3://sentence-data-ingestion/ML_EMBED_ASSETS/EMBED_META_FACT/finrag_fact_sentences_meta_embeds.parquet\n",
      "  Size: 33.6 MB\n",
      "  Downloaded: 469,252 rows (Cost: $0.0030 egress)\n",
      "  ✓ Cached to: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\meta_embeds\\finrag_fact_sentences_meta_embeds.parquet\n",
      "\n",
      "[Embeddings Fact - Downloading from S3] Provider: cohere_1024d\n",
      "  Source: s3://sentence-data-ingestion/ML_EMBED_ASSETS/EMBED_VECTORS/cohere_1024d/finrag_embeddings_cohere_1024d.parquet\n",
      "  Size: 16.0 MB\n",
      "  Downloaded: 8,966 rows (Cost: $0.0014 egress)\n",
      "  ✓ Cached to: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\embeddings\\cohere_1024d\\finrag_embeddings_cohere_1024d.parquet\n",
      "\n",
      "======================================================================\n",
      "✓ DATA PREPARATION COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Tables initialized / cached:\n",
      "  ✓ Stage 1 cached locally\n",
      "  ✓ Stage 2 meta cached locally\n",
      "  ✓ Embeddings cached locally → { cohere_1024d: 8966 rows }\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA PREPARATION PIPELINE\n",
    "# Creates Stage 2 meta table and initializes empty vectors table\n",
    "# ============================================================================\n",
    "# ============================================================================\n",
    "# PARAMETERS - Execution Control\n",
    "# INIT_*: Creates on S3 (one-time)\n",
    "# FORCE_REINIT_*: Recreates (destructive)\n",
    "# CACHE_*: Downloads locally\n",
    "# FORCE_RECACHE_*: Re-downloads\n",
    "# ============================================================================\n",
    "\n",
    "# S3 Table Initialization (One-time setup)\n",
    "INIT_META_TABLE = False           # Create Stage 2 meta table (24→35 cols)\n",
    "INIT_VECTORS_TABLE = False        # Create empty vectors table\n",
    "\n",
    "FORCE_REINIT_META = False        # Delete and recreate meta table\n",
    "FORCE_REINIT_VECTORS = False     # Delete and recreate vectors table\n",
    "\n",
    "# Local Caching (Development optimization)\n",
    "CACHE_STAGE1_LOCALLY = True      # Download Stage 1 fact table\n",
    "FORCE_RECACHE_STAGE1 = True     # Re-download even if cached\n",
    "\n",
    "CACHE_STAGE2_LOCALLY = True       # Download Stage 2 (meta_embeds) table\n",
    "FORCE_RECACHE_STAGE2 = True      # Re-download Stage 2 even if cached\n",
    "\n",
    "CACHE_EMBEDS_LOCALLY = True       # Download embeddings fact table(s)\n",
    "FORCE_RECACHE_EMBEDS = True      # Re-download embeddings even if cached\n",
    "\n",
    "EMBEDS_PROVIDER = \"cohere_1024d\"            # e.g. \"cohere_1024d\" (None → try sensible default or all)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from ml_config_loader import MLConfig\n",
    "import polars as pl\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def check_s3_exists(s3_client, bucket, s3_key):\n",
    "    \"\"\"Check if S3 object exists (no download, no cost)\"\"\"\n",
    "    try:\n",
    "        s3_client.head_object(Bucket=bucket, Key=s3_key)\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == '404':\n",
    "            return False\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "def _egress_cost_usd(bytes_count: int) -> float:\n",
    "    # Rough public egress reference: $0.09 per GB\n",
    "    gb = bytes_count / (1024 * 1024 * 1024)\n",
    "    return gb * 0.09\n",
    "\n",
    "def _download_parquet_from_s3_to_local(config, s3_key: str, local_path: Path) -> pl.DataFrame:\n",
    "    \"\"\"Read a parquet from S3 via Polars and cache locally (zstd).\"\"\"\n",
    "    s3_uri = f\"s3://{config.bucket}/{s3_key}\"\n",
    "    s3_client = config.get_s3_client()\n",
    "    head = s3_client.head_object(Bucket=config.bucket, Key=s3_key)\n",
    "    size_mb = head['ContentLength'] / 1024 / 1024\n",
    "    cost = _egress_cost_usd(head['ContentLength'])\n",
    "\n",
    "    print(f\"  Source: {s3_uri}\")\n",
    "    print(f\"  Size: {size_mb:.1f} MB\")\n",
    "    df = pl.read_parquet(s3_uri, storage_options=config.get_storage_options())\n",
    "    print(f\"  Downloaded: {len(df):,} rows (Cost: ${cost:.4f} egress)\")\n",
    "\n",
    "    local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.write_parquet(local_path, compression='zstd')\n",
    "    print(f\"  ✓ Cached to: {local_path}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========================================================================================================================\n",
    "# ========================================================================================================================\n",
    "\n",
    "\n",
    "def cache_stage1_table(config, force_recache=False):\n",
    "    \"\"\"\n",
    "    Download and cache Stage 1 fact table (original 24 columns)\n",
    "    Returns: DataFrame loaded from cache or S3\n",
    "    \"\"\"\n",
    "    \n",
    "    cache_file = Path.cwd().parent / 'data_cache' / 'stage1_facts' / 'finrag_fact_sentences.parquet'\n",
    "    \n",
    "    # Check local cache first\n",
    "    if not force_recache and cache_file.exists():\n",
    "        print(f\"\\n[Stage 1 Table - Using Cache]\")\n",
    "        print(f\"  Location: {cache_file.name}\")\n",
    "        df = pl.read_parquet(cache_file)\n",
    "        print(f\"  Loaded: {len(df):,} rows × {len(df.columns)} columns (Cost: $0.00)\")\n",
    "        return df\n",
    "    \n",
    "    # Download from S3\n",
    "    print(f\"\\n[Stage 1 Table - Downloading from S3]\")\n",
    "    s3_uri = f\"s3://{config.bucket}/{config.input_sentences_path}\"\n",
    "    \n",
    "    s3_client = config.get_s3_client()\n",
    "    response = s3_client.head_object(Bucket=config.bucket, Key=config.input_sentences_path)\n",
    "    file_size_mb = response['ContentLength'] / 1024 / 1024\n",
    "    egress_cost = file_size_mb / 1024 * 0.09\n",
    "    \n",
    "    print(f\"  Source: {s3_uri}\")\n",
    "    print(f\"  Size: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    df = pl.read_parquet(s3_uri, storage_options=config.get_storage_options())\n",
    "    print(f\"  Downloaded: {len(df):,} rows (Cost: ${egress_cost:.4f} egress)\")\n",
    "    \n",
    "    # Cache for future use\n",
    "    cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.write_parquet(cache_file, compression='zstd')\n",
    "    print(f\"  ✓ Cached to: {cache_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def cache_stage2_meta_table(config, force_recache=False):\n",
    "    \"\"\"\n",
    "    Download and cache Stage-2 meta table (35 columns with ML metadata).\n",
    "    Returns: DataFrame loaded from cache or S3.\n",
    "    \"\"\"\n",
    "    # Use the final filename from S3 key for local cache filename\n",
    "    meta_key = config.meta_embeds_path                          # e.g., \"ML_EMBED_ASSETS/EMBED_META_FACT/finrag_fact_sentences_meta_embeds.parquet\"\n",
    "    meta_filename = Path(meta_key).name\n",
    "    cache_file = Path.cwd().parent / 'data_cache' / 'meta_embeds' / meta_filename\n",
    "\n",
    "    if not force_recache and cache_file.exists():\n",
    "        print(f\"\\n[Stage 2 Meta Table - Using Cache]\")\n",
    "        print(f\"  Location: {cache_file.name}\")\n",
    "        df = pl.read_parquet(cache_file)\n",
    "        print(f\"  Loaded: {len(df):,} rows × {len(df.columns)} columns (Cost: $0.00)\")\n",
    "        return df\n",
    "\n",
    "    print(f\"\\n[Stage 2 Meta Table - Downloading from S3]\")\n",
    "    return _download_parquet_from_s3_to_local(config, meta_key, cache_file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _resolve_embed_providers(config, explicit: str | None):\n",
    "    \"\"\"\n",
    "    Try to resolve provider list:\n",
    "    - If explicit provided → [explicit]\n",
    "    - Else, try config.providers if exposed\n",
    "    - Else, fall back to default model key (single)\n",
    "    \"\"\"\n",
    "    if explicit:\n",
    "        return [explicit]\n",
    "\n",
    "    # If MLConfig exposes a list of providers (preferred)\n",
    "    if hasattr(config, \"embedding_providers\") and isinstance(config.embedding_providers, (list, tuple)) and config.embedding_providers:\n",
    "        return list(config.embedding_providers)\n",
    "\n",
    "    # Fallback: try default model key\n",
    "    if hasattr(config, \"bedrock_default_model_key\") and config.bedrock_default_model_key:\n",
    "        return [config.bedrock_default_model_key]\n",
    "\n",
    "    # As a last resort, try a few common keys the YAML showed (only if present in config)\n",
    "    candidates = [k for k in (\"cohere_768d\", \"cohere_1024d\", \"titan_1024d\") if hasattr(config, \"embeddings_path\") and config.embeddings_path(provider=k)]\n",
    "    return candidates or []\n",
    "\n",
    "\n",
    "def cache_embeddings_tables(config, providers=None, force_recache=False):\n",
    "    \"\"\"\n",
    "    Download and cache embeddings parquet for one or many providers.\n",
    "    Returns: dict {provider: DataFrame}\n",
    "    \"\"\"\n",
    "    if providers is None:\n",
    "        providers = _resolve_embed_providers(config, explicit=EMBEDS_PROVIDER)\n",
    "\n",
    "    results = {}\n",
    "    if not providers:\n",
    "        print(\"\\n[Embeddings Fact - No providers resolved] Skipping.\")\n",
    "        return results\n",
    "\n",
    "    for prov in providers:\n",
    "        # Expect MLConfig to resolve full S3 key for the provider's embeddings fact parquet\n",
    "        # e.g., \"ML_EMBED_ASSETS/EMBED_VECTORS/cohere_1024d/finrag_embeddings_cohere_1024d.parquet\"\n",
    "        try:\n",
    "            emb_key = config.embeddings_path(provider=prov)\n",
    "        except TypeError:\n",
    "            # If embeddings_path does not accept provider kw, try attribute\n",
    "            emb_key = getattr(config, \"embeddings_path\", None)\n",
    "        if not emb_key:\n",
    "            print(f\"\\n[Embeddings Fact - {prov}] Cannot resolve S3 key via config.embeddings_path(provider=...). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        emb_filename = Path(emb_key).name\n",
    "        cache_file = Path.cwd().parent / 'data_cache' / 'embeddings' / prov / emb_filename\n",
    "\n",
    "        if not force_recache and cache_file.exists():\n",
    "            print(f\"\\n[Embeddings Fact - Using Cache] Provider: {prov}\")\n",
    "            print(f\"  Location: {emb_filename}\")\n",
    "            df = pl.read_parquet(cache_file)\n",
    "            print(f\"  Loaded: {len(df):,} rows × {len(df.columns)} columns (Cost: $0.00)\")\n",
    "            results[prov] = df\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[Embeddings Fact - Downloading from S3] Provider: {prov}\")\n",
    "        df = _download_parquet_from_s3_to_local(config, emb_key, cache_file)\n",
    "        results[prov] = df\n",
    "\n",
    "    return results\n",
    "\n",
    "# ========================================================================================================================\n",
    "# ========================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_ml_columns(df):\n",
    "    \"\"\"Transform Stage 1 (24 cols) → Stage 2 (35 cols) with ML metadata\"\"\"\n",
    "    \n",
    "    # Extract sequence for sorting\n",
    "    df = df.with_columns([\n",
    "        pl.col('sentenceID').str.split('_').list.slice(0, -1).list.join('_').alias('_doc_prefix'),\n",
    "        pl.col('sentenceID').str.split('_').list.last().cast(pl.Int32).alias('_sequence_num')\n",
    "    ])\n",
    "    \n",
    "    df = df.sort(['_doc_prefix', '_sequence_num'])\n",
    "    \n",
    "    # Neighbor pointers\n",
    "    df = df.with_columns([\n",
    "        pl.col('sentenceID').shift(1).over('_doc_prefix').alias('prev_sentenceID'),\n",
    "        pl.col('sentenceID').shift(-1).over('_doc_prefix').alias('next_sentenceID')\n",
    "    ])\n",
    "    \n",
    "    # Content metadata\n",
    "    df = df.with_columns([\n",
    "        pl.col('sentence').str.len_chars().alias('sentence_char_length'),\n",
    "        ((pl.col('sentence').str.count_matches(' ') + 1) * 1.33).cast(pl.Int16).alias('sentence_token_count')\n",
    "    ])\n",
    "    \n",
    "    # Section metadata\n",
    "    section_counts = df.group_by(['docID', 'section_ID']).agg(pl.len().alias('section_sentence_count'))\n",
    "    df = df.join(section_counts, on=['docID', 'section_ID'], how='left')\n",
    "    \n",
    "    # ML metadata (NULL initially)\n",
    "    df = df.with_columns([\n",
    "        pl.lit(None).cast(pl.Utf8).alias('embedding_id'),\n",
    "        pl.lit(None).cast(pl.Utf8).alias('embedding_model'),\n",
    "        pl.lit(None).cast(pl.Int16).alias('embedding_dims'),\n",
    "        pl.lit(None).cast(pl.Datetime).alias('embedding_date'),\n",
    "        pl.lit(None).cast(pl.Utf8).alias('embedding_ref')\n",
    "    ])\n",
    "    \n",
    "    # Cleanup\n",
    "    df = df.drop(['_doc_prefix', '_sequence_num'])\n",
    "    \n",
    "    print(f\"  ✓ Transformed: 24 cols → {len(df.columns)} cols\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ========================================================================================================================\n",
    "# ========================================================================================================================\n",
    "\n",
    "\n",
    "def initialize_meta_table(config, df_stage1, force_reinit=False):\n",
    "    \"\"\"Create Stage 2 meta table on S3 (35 columns)\"\"\"\n",
    "    \n",
    "    s3_client = config.get_s3_client()\n",
    "    meta_s3_key = config.meta_embeds_path\n",
    "    meta_exists = check_s3_exists(s3_client, config.bucket, meta_s3_key)\n",
    "    \n",
    "    # Handle existing table\n",
    "    if meta_exists and not force_reinit:\n",
    "        print(f\"\\n[Stage 2 Meta Table - Already Exists]\")\n",
    "        print(f\"  Location: s3://{config.bucket}/{meta_s3_key}\")\n",
    "        print(f\"  Set FORCE_REINIT_META=True to recreate\")\n",
    "        return\n",
    "    \n",
    "    elif meta_exists and force_reinit:\n",
    "        print(f\"\\n[Stage 2 Meta Table - Recreating]\")\n",
    "        s3_client.delete_object(Bucket=config.bucket, Key=meta_s3_key)\n",
    "        print(f\"  ✓ Deleted existing\")\n",
    "    \n",
    "    # Create new table\n",
    "    print(f\"\\n[Stage 2 Meta Table - Creating]\")\n",
    "    df_stage2 = add_ml_columns(df_stage1)\n",
    "    \n",
    "    meta_uri = f\"s3://{config.bucket}/{meta_s3_key}\"\n",
    "    print(f\"  Saving to: {meta_uri}\")\n",
    "    df_stage2.write_parquet(meta_uri, storage_options=config.get_storage_options(), compression='zstd')\n",
    "    print(f\"  ✓ Created: {len(df_stage2):,} rows × {len(df_stage2.columns)} cols (Cost: $0.00 ingress)\")\n",
    "\n",
    "\n",
    "def initialize_vectors_table(config, force_reinit=False):\n",
    "    \"\"\"Create empty vectors table on S3\"\"\"\n",
    "    \n",
    "    s3_client = config.get_s3_client()\n",
    "    vectors_s3_key = config.embeddings_path(provider=None)\n",
    "    vectors_exists = check_s3_exists(s3_client, config.bucket, vectors_s3_key)\n",
    "    \n",
    "    # Handle existing table\n",
    "    if vectors_exists and not force_reinit:\n",
    "        print(f\"\\n[Vectors Table - Already Exists]\")\n",
    "        print(f\"  Location: s3://{config.bucket}/{vectors_s3_key}\")\n",
    "        print(f\"  Provider: {config.bedrock_default_model_key} ({config.bedrock_dimensions}d)\")\n",
    "        print(f\"  Set FORCE_REINIT_VECTORS=True to recreate\")\n",
    "        return\n",
    "    \n",
    "    elif vectors_exists and force_reinit:\n",
    "        print(f\"\\n[Vectors Table - Recreating]\")\n",
    "        s3_client.delete_object(Bucket=config.bucket, Key=vectors_s3_key)\n",
    "        print(f\"  ✓ Deleted existing\")\n",
    "    \n",
    "    # Create empty table\n",
    "    print(f\"\\n[Vectors Table - Creating]\")\n",
    "    empty_vectors = pl.DataFrame({\n",
    "        'sentenceID': pl.Series([], dtype=pl.Utf8),\n",
    "        'embedding_id': pl.Series([], dtype=pl.Utf8),\n",
    "        'embedding': pl.Series([], dtype=pl.List(pl.Float32))\n",
    "    })\n",
    "    \n",
    "    vectors_uri = f\"s3://{config.bucket}/{vectors_s3_key}\"\n",
    "    empty_vectors.write_parquet(vectors_uri, storage_options=config.get_storage_options(), compression='zstd')\n",
    "    print(f\"  ✓ Created: {vectors_uri}\")\n",
    "    print(f\"  Schema: [sentenceID, embedding_id, embedding (Float32)]\")\n",
    "\n",
    "# ========================================================================================================================\n",
    "# ========================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "config = MLConfig()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA PREPARATION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {config.bedrock_model_id} ({config.bedrock_dimensions}d)\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Cache Stage 1 Table Locally\n",
    "# ============================================================================\n",
    "\n",
    "df_stage1 = None\n",
    "\n",
    "if CACHE_STAGE1_LOCALLY:\n",
    "    df_stage1 = cache_stage1_table(config, force_recache=FORCE_RECACHE_STAGE1)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Cache Stage 2 Meta Table Locally\n",
    "# ============================================================================\n",
    "\n",
    "df_stage2_meta = None\n",
    "if CACHE_STAGE2_LOCALLY:\n",
    "    df_stage2_meta = cache_stage2_meta_table(config, force_recache=FORCE_RECACHE_STAGE2)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 1: Cache Embeddings Fact Table(s) Locally\n",
    "# ============================================================================\n",
    "\n",
    "embeds_cached = {}\n",
    "if CACHE_EMBEDS_LOCALLY:\n",
    "    # If EMBEDS_PROVIDER is None, resolver will try providers list or default model\n",
    "    embeds_cached = cache_embeddings_tables(config, providers=None, force_recache=FORCE_RECACHE_EMBEDS)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 2: Initialize Meta Table (Stage 2) on S3\n",
    "# ============================================================================\n",
    "\n",
    "if INIT_META_TABLE:\n",
    "    if df_stage1 is None:\n",
    "        print(f\"\\n  Loading Stage 1 for transformation...\")\n",
    "        df_stage1 = cache_stage1_table(config, force_recache=False)\n",
    "    initialize_meta_table(config, df_stage1, force_reinit=FORCE_REINIT_META)\n",
    "\n",
    "# ============================================================================\n",
    "# TASK 3: Initialize Empty Vectors Table on S3\n",
    "# ============================================================================\n",
    "\n",
    "if INIT_VECTORS_TABLE:\n",
    "    initialize_vectors_table(config, force_reinit=FORCE_REINIT_VECTORS)\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ DATA PREPARATION COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\nTables initialized / cached:\")\n",
    "if CACHE_STAGE1_LOCALLY:\n",
    "    print(f\"  ✓ Stage 1 cached locally\")\n",
    "if CACHE_STAGE2_LOCALLY:\n",
    "    print(f\"  ✓ Stage 2 meta cached locally\")\n",
    "if CACHE_EMBEDS_LOCALLY:\n",
    "    if embeds_cached:\n",
    "        prows = \", \".join([f\"{k}: {len(v)} rows\" for k, v in embeds_cached.items()])\n",
    "        print(f\"  ✓ Embeddings cached locally → {{ {prows} }}\")\n",
    "    else:\n",
    "        print(f\"  ✓ Embeddings cache attempted (no providers resolved)\")\n",
    "if INIT_META_TABLE:\n",
    "    print(f\"  ✓ Stage 2 meta table ready (S3)\")\n",
    "if INIT_VECTORS_TABLE:\n",
    "    print(f\"  ✓ Vectors table ready (S3)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3495f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc90fb15",
   "metadata": {},
   "source": [
    "### Embedding Generation, Storage and Push, & Embedding metadata update in main fact_sentences_meta_embed table\n",
    "\n",
    "- At filtering step:\n",
    "   - `filtered_sentence_ids = df_filtered['sentenceID'].to_list()`\n",
    "- List is your anchor throughout the pipeline\n",
    "   - Knowing which sentences to embed. Knowing which meta rows to update. Progress tracking. Cost estimation.\n",
    "- Merge.\n",
    "  - `merged = pl.concat([existing_df, new_df])` \n",
    "  - `merged = merged.unique(subset=[key], keep='last') `\n",
    "\n",
    "- p50 (median): 33 tokens\n",
    "- p75: 48 tokens\n",
    "- p95: 77 tokens\n",
    "- p99: 117 tokens\n",
    "- max: 2,281 tokens (outlier - likely table)\n",
    "- 99.7% of sentences: <500 tokens\n",
    "\n",
    "- Bedrock Cohere v4 limits:\n",
    "   - Max tokens per text: 512 tokens\n",
    "   - Max texts per batch: 96\n",
    "   - Max total request tokens: ~50K (undocumented, but conservative estimate)\n",
    "\n",
    "### Right on aws cohere page: \"Smaller chunks improve retrieval and cost\" \n",
    "- Long chunks: More tokens = higher embedding cost\n",
    "- Short chunks: Fewer tokens = lower cost\n",
    "-   1,000 chunks × 500 tokens each = 500K tokens → $0.05\n",
    "-   1,000 chunks × 50 tokens each = 50K tokens → $0.005\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "# TOP: Constants & Path Resolution\n",
    "config = MLConfig()\n",
    "VECTORS_URI = ...\n",
    "META_URI = ...\n",
    "\n",
    "# STEP 1: Load meta table (helper)\n",
    "df_meta = load_meta_table_with_cache(config)\n",
    "\n",
    "# STEP 2: Filter (returns anchor)\n",
    "df_filtered, filtered_ids = filter_sentences(df_meta, config)\n",
    "\n",
    "# STEP 3: Generate embeddings\n",
    "df_vectors, embedding_id, skipped_ids = generate_embeddings_batch(...)\n",
    "\n",
    "# STEP 4: Merge vectors (simplified - no existence checks)\n",
    "merged_vectors = merge_vectors_table(df_vectors, VECTORS_URI, storage_options)\n",
    "\n",
    "# STEP 5: Update meta (use anchor)\n",
    "updated_meta = update_meta_table(df_meta, filtered_ids, skipped_ids, model_info)\n",
    "\n",
    "# STEP 6: Save both\n",
    "Save vectors\n",
    "Save meta\n",
    "\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "- EMBEDS:\n",
    "- APPLE; 2016? -> // → 47,755 tokens processed → Cost: $0.0048\n",
    "\n",
    "- Actual file on S3: 21MB (compressed with ZSTD)\n",
    "- EGRESS: code overstated the cost by 10x // 21MB / 1024 × $0.09 = $0.0018 (not $0.0246)\n",
    "\n",
    "data_cache/stage1_facts/ → Stage-1 parquet\n",
    "data_cache/meta_embeds/ → Stage-2 meta (35 cols)\n",
    "data_cache/embeddings/<provider>/ → vectors per provider (e.g., cohere_1024d/)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c3bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] ✓ AWS credentials loaded from aws_credentials.env\n",
      "======================================================================\n",
      "EMBEDDING GENERATION PIPELINE\n",
      "======================================================================\n",
      "Mode: parameterized\n",
      "Model: cohere.embed-v4:0 (1024d)\n",
      "\n",
      "[Resolved Paths]\n",
      "  Vectors: ML_EMBED_ASSETS/EMBED_VECTORS/cohere_1024d/finrag_embeddings_cohere_1024d.parquet\n",
      "  Meta: ML_EMBED_ASSETS/EMBED_META_FACT/finrag_fact_sentences_meta_embeds.parquet\n",
      "✓ Using cached meta table\n",
      "  Loaded: 469,252 rows × 34 columns (Cost: $0.00)\n",
      "\n",
      "[Filtering: PARAMETERIZED MODE]\n",
      "  CIKs: [1276520, 1318605, 1326801, 1341439, 1403161, 1652044]\n",
      "  Years: [2015, 2016, 2017, 2018, 2019, 2020]\n",
      "  Companies selected:\n",
      "    - GENWORTH FINANCIAL INC (CIK: 1276520)\n",
      "    - Tesla, Inc. (CIK: 1318605)\n",
      "    - Meta Platforms, Inc. (CIK: 1326801)\n",
      "    - Facebook Inc (CIK: 1326801)\n",
      "    - ORACLE CORP (CIK: 1341439)\n",
      "    - VISA INC. (CIK: 1403161)\n",
      "    - Alphabet Inc. (CIK: 1652044)\n",
      "  Total sentences: 69,512\n",
      "\n",
      "[Generating Embeddings]\n",
      "  Model: cohere.embed-v4:0\n",
      "  Dimensions: 1024\n",
      "  Input sentences: 69,512\n",
      "  ⚠️  Skipped 5 outliers (>1000 tokens, 0.01%)\n",
      "  Embedding: 69,507 sentences\n",
      "    Batch Number: 40 | Progress: 3,840/69,507 (5.5%) | last 1.06s | avg/batch 1.14s | ETA 780s\n",
      "    Batch Number: 80 | Progress: 7,680/69,507 (11.0%) | last 1.04s | avg/batch 1.09s | ETA 700s\n",
      "    Batch Number: 120 | Progress: 11,520/69,507 (16.6%) | last 0.91s | avg/batch 1.06s | ETA 641s\n",
      "    Batch Number: 160 | Progress: 15,360/69,507 (22.1%) | last 1.25s | avg/batch 1.10s | ETA 619s\n",
      "    Batch Number: 200 | Progress: 19,200/69,507 (27.6%) | last 1.01s | avg/batch 1.17s | ETA 613s\n",
      "    Batch Number: 240 | Progress: 23,040/69,507 (33.1%) | last 1.24s | avg/batch 1.21s | ETA 585s\n",
      "    Batch Number: 280 | Progress: 26,880/69,507 (38.7%) | last 1.05s | avg/batch 1.25s | ETA 555s\n",
      "    Batch Number: 320 | Progress: 30,720/69,507 (44.2%) | last 0.96s | avg/batch 1.28s | ETA 518s\n",
      "    Batch Number: 360 | Progress: 34,560/69,507 (49.7%) | last 1.34s | avg/batch 1.30s | ETA 473s\n",
      "    Batch Number: 400 | Progress: 38,400/69,507 (55.2%) | last 0.97s | avg/batch 1.31s | ETA 425s\n",
      "    Batch Number: 440 | Progress: 42,240/69,507 (60.8%) | last 1.08s | avg/batch 1.33s | ETA 376s\n",
      "    Batch Number: 480 | Progress: 46,080/69,507 (66.3%) | last 1.05s | avg/batch 1.34s | ETA 326s\n",
      "    Batch Number: 520 | Progress: 49,920/69,507 (71.8%) | last 1.77s | avg/batch 1.35s | ETA 276s\n",
      "    Batch Number: 560 | Progress: 53,760/69,507 (77.3%) | last 2.89s | avg/batch 1.36s | ETA 223s\n",
      "    Batch Number: 600 | Progress: 57,600/69,507 (82.9%) | last 0.97s | avg/batch 1.37s | ETA 169s\n",
      "    Batch Number: 640 | Progress: 61,440/69,507 (88.4%) | last 1.06s | avg/batch 1.37s | ETA 115s\n",
      "    Batch Number: 680 | Progress: 65,280/69,507 (93.9%) | last 1.01s | avg/batch 1.38s | ETA 61s\n",
      "    Batch Number: 720 | Progress: 69,120/69,507 (99.4%) | last 1.45s | avg/batch 1.38s | ETA 6s\n",
      "  ✓ Completed: 69,507 embeddings in 725 batches | time 1001.4s | avg/batch 1.38s\n",
      "  Tokens: 2,693,551 | Cost: $0.2694\n",
      "\n",
      "[Merging Vectors]\n",
      "  Existing: 147,767 rows\n",
      "  New: 69,507 rows\n",
      "  ✓ Merged: 203,076 rows (replaced 14,198)\n",
      "\n",
      "[Updating Meta Table]\n",
      "  Total rows: 469,252\n",
      "  Successfully embedded: 69,507\n",
      "  Skipped (outliers): 5\n",
      "  ✓ Updated: 203,076 total rows now have embeddings\n",
      "\n",
      "[Saving Results]\n",
      "  Vectors → S3: s3://sentence-data-ingestion/ML_EMBED_ASSETS/EMBED_VECTORS/cohere_1024d/finrag_embeddings_cohere_1024d.parquet\n",
      "  ✓ S3 saved: 203,076 rows (Cost: $0.00 - ingress)\n",
      "  Vectors → Local: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\embeddings\\cohere_1024d\\finrag_embeddings_cohere_1024d.parquet\n",
      "  ✓ Cached locally\n",
      "  Meta → S3: s3://sentence-data-ingestion/ML_EMBED_ASSETS/EMBED_META_FACT/finrag_fact_sentences_meta_embeds.parquet\n",
      "  ✓ S3 saved: 469,252 rows (Cost: $0.00 - ingress)\n",
      "  Meta → Local: d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\meta_embeds\\finrag_fact_sentences_meta_embeds.parquet\n",
      "  ✓ Cached locally\n",
      "\n",
      "  Local cache locations:\n",
      "    - d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\embeddings\\cohere_1024d\\finrag_embeddings_cohere_1024d.parquet\n",
      "    - d:\\JoelDesktop folds_24\\NEU FALL2025\\MLops IE7374 Project\\FinSights\\ModelPipeline\\finrag_ml_tg1\\data_cache\\meta_embeds\\finrag_fact_sentences_meta_embeds.parquet\n",
      "\n",
      "======================================================================\n",
      "✓ EMBEDDING PIPELINE COMPLETE\n",
      "======================================================================\n",
      "  Mode: parameterized\n",
      "  Sentences embedded: 69,507\n",
      "  Sentences skipped: 5\n",
      "  Total vectors in storage: 203,076\n",
      "  Total meta rows with embeddings: 203,076\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EMBEDDING GENERATION PIPELINE\n",
    "# Generates embeddings for filtered sentences and merges with existing data\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "COHERE SPECIFIC:\n",
    "    Batch fills when EITHER condition met:\n",
    "    1. 96 texts reached, OR\n",
    "    2. Total tokens reached\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# PIPELINE CONSTANTS\n",
    "MAX_TOKENS_PER_SENTENCE = 1000     # Filter extreme outliers\n",
    "\n",
    "# MAX_TEXTS_PER_BATCH = 96           # API limit\n",
    "# MAX_TOKENS_PER_BATCH = 128000       # PREV used 15000\n",
    "\n",
    "MAX_TEXTS_PER_BATCH = 96           \n",
    "MAX_TOKENS_PER_BATCH = 128000        # Cohere v4's maximum capacity (128K tokens)\n",
    "\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'loaders'))\n",
    "\n",
    "from loaders.ml_config_loader import MLConfig\n",
    "import polars as pl\n",
    "import json\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def load_meta_table_with_cache(config):\n",
    "    \"\"\"Load meta table (cache first, S3 fallback, error if missing)\"\"\"\n",
    "    \n",
    "    cache_file = Path.cwd().parent / 'data_cache' / 'meta_embeds' / 'finrag_fact_sentences_meta_embeds.parquet'\n",
    "    \n",
    "    if cache_file.exists():\n",
    "        print(f\"✓ Using cached meta table\")\n",
    "        df = pl.read_parquet(cache_file)\n",
    "        print(f\"  Loaded: {len(df):,} rows × {len(df.columns)} columns (Cost: $0.00)\")\n",
    "        return df\n",
    "    \n",
    "    # Try S3\n",
    "    meta_uri = f\"s3://{config.bucket}/{config.meta_embeds_path}\"\n",
    "    s3_client = config.get_s3_client()\n",
    "\n",
    "    try:\n",
    "        response = s3_client.head_object(Bucket=config.bucket, Key=config.meta_embeds_path)\n",
    "        file_size_bytes = response['ContentLength']\n",
    "        file_size_mb = file_size_bytes / 1024 / 1024\n",
    "        egress_cost = file_size_mb / 1024 * 0.09\n",
    "\n",
    "        print(f\"⬇️  Loading meta table from S3\")\n",
    "        df = pl.read_parquet(meta_uri, storage_options=config.get_storage_options())\n",
    "        \n",
    "        print(f\"  Downloaded: {len(df):,} rows × {len(df.columns)} columns\")\n",
    "        print(f\"  Cost: ${egress_cost:.4f} egress\")\n",
    "        \n",
    "        # Cache for next time\n",
    "        cache_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.write_parquet(cache_file)\n",
    "        print(f\"✓ Cached for future use\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Meta table not found!\\n\"\n",
    "            f\"  Cache: {cache_file}\\n\"\n",
    "            f\"  S3: {meta_uri}\\n\"\n",
    "            f\"  → Run '01_dataprep.ipynb' first to create meta table\"\n",
    "        )\n",
    "\n",
    "\n",
    "def filter_sentences(df_meta, config):\n",
    "    \"\"\"Filter sentences with multi-value support for CIK and year\"\"\"\n",
    "    \n",
    "    mode = config.embedding_mode\n",
    "    \n",
    "    if mode == \"full\":\n",
    "        print(f\"\\n[Filtering: FULL MODE]\")\n",
    "        df_filtered = df_meta\n",
    "    \n",
    "    elif mode == \"parameterized\":\n",
    "        cik_filter = config.filter_cik\n",
    "        year_filter = config.filter_year\n",
    "        \n",
    "        # Validate filters are not null\n",
    "        if cik_filter is None or year_filter is None:\n",
    "            raise ValueError(\n",
    "                \"PARAMETERIZED mode requires filters!\\n\"\n",
    "                \"  Set 'cik_int' and 'year' in ml_config.yaml\\n\"\n",
    "                \"  Example: cik_int: [320193] or cik_int: [320193, 789019]\\n\"\n",
    "                \"  Example: year: [2022] or year: [2020, 2021, 2022]\"\n",
    "            )\n",
    "        \n",
    "        # Ensure filters are lists\n",
    "        cik_list = cik_filter if isinstance(cik_filter, list) else [cik_filter]\n",
    "        year_list = year_filter if isinstance(year_filter, list) else [year_filter]\n",
    "        \n",
    "        print(f\"\\n[Filtering: PARAMETERIZED MODE]\")\n",
    "        print(f\"  CIKs: {cik_list}\")\n",
    "        print(f\"  Years: {year_list}\")\n",
    "        \n",
    "        # Apply filters\n",
    "        df_filtered = df_meta.filter(\n",
    "            pl.col('cik_int').is_in(cik_list) &\n",
    "            pl.col('report_year').is_in(year_list)\n",
    "        )\n",
    "        \n",
    "        if len(df_filtered) == 0:\n",
    "            raise ValueError(f\"No sentences found for cik={cik_list}, year={year_list}\")\n",
    "        \n",
    "        # Show selected companies\n",
    "        companies = df_filtered.select(['cik_int', 'name']).unique().sort('cik_int')\n",
    "        print(f\"  Companies selected:\")\n",
    "        for row in companies.iter_rows(named=True):\n",
    "            print(f\"    - {row['name']} (CIK: {row['cik_int']})\")\n",
    "        \n",
    "        print(f\"  Total sentences: {len(df_filtered):,}\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "    \n",
    "    filtered_ids = df_filtered['sentenceID'].to_list()\n",
    "    \n",
    "    return df_filtered, filtered_ids\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_embeddings_batch(df_sentences, config):\n",
    "    \"\"\"\n",
    "    Generate embeddings with intelligent batching and outlier handling\n",
    "    \n",
    "    Returns:\n",
    "      - df_vectors: DataFrame with [sentenceID, embedding_id, embedding]\n",
    "      - embedding_id: Unique ID for this embedding run\n",
    "      - skipped_ids: List of sentenceIDs that were filtered out\n",
    "    \"\"\"\n",
    "    \n",
    "    bedrock = config.get_bedrock_client()\n",
    "    model_id = config.bedrock_model_id\n",
    "    input_type = config.bedrock_input_type\n",
    "    dimensions = config.bedrock_dimensions\n",
    "    \n",
    "    print(f\"\\n[Generating Embeddings]\")\n",
    "    print(f\"  Model: {model_id}\")\n",
    "    print(f\"  Dimensions: {dimensions}\")\n",
    "    print(f\"  Input sentences: {len(df_sentences):,}\")\n",
    "    \n",
    "    # Pre-filter: Remove extreme outliers\n",
    "    df_valid = df_sentences.filter(\n",
    "        pl.col('sentence_token_count') <= MAX_TOKENS_PER_SENTENCE\n",
    "    )\n",
    "    \n",
    "    df_skipped = df_sentences.filter(\n",
    "        pl.col('sentence_token_count') > MAX_TOKENS_PER_SENTENCE\n",
    "    )\n",
    "    \n",
    "    if len(df_skipped) > 0:\n",
    "        skipped_pct = len(df_skipped) / len(df_sentences) * 100\n",
    "        print(f\"  ⚠️  Skipped {len(df_skipped):,} outliers (>{MAX_TOKENS_PER_SENTENCE} tokens, {skipped_pct:.2f}%)\")\n",
    "    \n",
    "    print(f\"  Embedding: {len(df_valid):,} sentences\")\n",
    "    \n",
    "    # Prepare for batching\n",
    "    all_embeddings = []\n",
    "    all_sentence_ids = []\n",
    "    sentences_data = df_valid.select(['sentenceID', 'sentence', 'sentence_token_count']).to_dicts()\n",
    "    \n",
    "    current_batch = []\n",
    "    current_batch_tokens = 0\n",
    "    batches_processed = 0\n",
    "    total_tokens = 0\n",
    "    # timing trackers\n",
    "    t0 = time.perf_counter()        \n",
    "    batch_times = []\n",
    "    batch_log_interval = 40                \n",
    "\n",
    "    # Token-aware batching\n",
    "    for idx, row in enumerate(sentences_data):\n",
    "        sent_id = row['sentenceID']\n",
    "        sent_text = row['sentence']\n",
    "        sent_tokens = row['sentence_token_count']\n",
    "        \n",
    "        # Check if batch is full\n",
    "        would_exceed_tokens = (current_batch_tokens + sent_tokens) > MAX_TOKENS_PER_BATCH\n",
    "        would_exceed_size = len(current_batch) >= MAX_TEXTS_PER_BATCH\n",
    "        \n",
    "        if would_exceed_tokens or would_exceed_size:\n",
    "            # Process current batch\n",
    "            if current_batch:\n",
    "                b_start = time.perf_counter()\n",
    "                batch_embeddings = _call_bedrock_api(\n",
    "                    bedrock, model_id, current_batch, input_type, dimensions\n",
    "                )\n",
    "                batch_time = time.perf_counter() - b_start   \n",
    "                batch_times.append(batch_time)               # <- track batch time\n",
    "\n",
    "\n",
    "                all_embeddings.extend(batch_embeddings)\n",
    "                batches_processed += 1\n",
    "                total_tokens += current_batch_tokens\n",
    "                \n",
    "                # Progress every n batches\n",
    "                if batches_processed % batch_log_interval == 0:\n",
    "                    avg = sum(batch_times) / len(batch_times)\n",
    "                    eta = avg * ((len(df_valid) / len(current_batch)) - batches_processed)\n",
    "                    print(f\"    Batch Number: {batches_processed} | Progress: {len(all_embeddings):,}/{len(df_valid):,} \"\n",
    "                          f\"({len(all_embeddings)/len(df_valid)*100:.1f}%) \"\n",
    "                          f\"| last {batch_time:.2f}s | avg/batch {avg:.2f}s | ETA {eta:.0f}s\")\n",
    "            \n",
    "            # Reset batch\n",
    "            current_batch = []\n",
    "            current_batch_tokens = 0\n",
    "        \n",
    "        # Add to batch\n",
    "        current_batch.append({'id': sent_id, 'text': sent_text})\n",
    "        current_batch_tokens += sent_tokens\n",
    "        all_sentence_ids.append(sent_id)\n",
    "    \n",
    "    # Final batch\n",
    "    if current_batch:\n",
    "        b_start = time.perf_counter()              # <-- added\n",
    "        batch_embeddings = _call_bedrock_api(\n",
    "            bedrock, model_id, current_batch, input_type, dimensions\n",
    "        )\n",
    "        batch_time = time.perf_counter() - b_start # <-- added\n",
    "        batch_times.append(batch_time)             # <-- added\n",
    "\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        batches_processed += 1\n",
    "        total_tokens += current_batch_tokens\n",
    "\n",
    "    elapsed = time.perf_counter() - t0             # <-- added\n",
    "    print(f\"  ✓ Completed: {len(all_embeddings):,} embeddings in {batches_processed} batches \"\n",
    "          f\"| time {elapsed:.1f}s | avg/batch {elapsed/max(1,batches_processed):.2f}s\")  # <-- added\n",
    "\n",
    "    embedding_id = f\"bedrock_cohere_v4_{dimensions}d_{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    df_vectors = pl.DataFrame({\n",
    "        'sentenceID': all_sentence_ids,\n",
    "        'embedding_id': [embedding_id] * len(all_embeddings),\n",
    "        'embedding': pl.Series(all_embeddings, dtype=pl.List(pl.Float32))\n",
    "    })\n",
    "\n",
    "    cost = total_tokens / 1000 * config.get_cost_per_1k()\n",
    "    print(f\"  Tokens: {total_tokens:,} | Cost: ${cost:.4f}\")\n",
    "\n",
    "    skipped_ids = df_skipped['sentenceID'].to_list() if len(df_skipped) > 0 else []\n",
    "    return df_vectors, embedding_id, skipped_ids\n",
    "\n",
    "\n",
    "\n",
    "def _call_bedrock_api(bedrock, model_id, batch, input_type, dimensions):\n",
    "    \"\"\"Single Bedrock API call (internal helper)\"\"\"\n",
    "    \n",
    "    texts = [item['text'] for item in batch]\n",
    "    \n",
    "    body = json.dumps({\n",
    "        \"texts\": texts,\n",
    "        \"input_type\": input_type,\n",
    "        \"embedding_types\": [\"float\"],\n",
    "        \"output_dimension\": dimensions,\n",
    "        \"max_tokens\": 128000,      #: Per-input token budget\n",
    "        \"truncate\": \"RIGHT\"        #: Safety for edge cases\n",
    "    })\n",
    "    \n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept='*/*',\n",
    "        contentType='application/json'\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['body'].read())\n",
    "    return result['embeddings']['float']\n",
    "\n",
    "\n",
    "def merge_vectors_table(new_vectors, vectors_uri, storage_options):\n",
    "    \"\"\"Merge new vectors with existing (table exists from data prep)\"\"\"\n",
    "    \n",
    "    # Load existing (guaranteed to exist)\n",
    "    existing_vectors = pl.read_parquet(vectors_uri, storage_options=storage_options)\n",
    "    \n",
    "    print(f\"\\n[Merging Vectors]\")\n",
    "    print(f\"  Existing: {len(existing_vectors):,} rows\")\n",
    "    print(f\"  New: {len(new_vectors):,} rows\")\n",
    "    \n",
    "    # Ensure column order matches before concat\n",
    "    new_vectors = new_vectors.select(existing_vectors.columns)\n",
    "\n",
    "    # ETL pattern: concat + unique\n",
    "    merged = pl.concat([existing_vectors, new_vectors])\n",
    "    merged = merged.unique(subset=['sentenceID'], keep='last')\n",
    "    \n",
    "    duplicates = len(existing_vectors) + len(new_vectors) - len(merged)\n",
    "    print(f\"  ✓ Merged: {len(merged):,} rows (replaced {duplicates:,})\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n",
    "def update_meta_table(df_meta_full, filtered_ids, skipped_ids, model_info, vectors_s3_uri):\n",
    "    \"\"\"Update metadata columns for embedded sentences (ETL pattern)\"\"\"\n",
    "    \n",
    "    # Successfully embedded = filtered - skipped\n",
    "    embedded_ids = [sid for sid in filtered_ids if sid not in skipped_ids]\n",
    "    \n",
    "    print(f\"\\n[Updating Meta Table]\")\n",
    "    print(f\"  Total rows: {len(df_meta_full):,}\")\n",
    "    print(f\"  Successfully embedded: {len(embedded_ids):,}\")\n",
    "    if skipped_ids:\n",
    "        print(f\"  Skipped (outliers): {len(skipped_ids):,}\")\n",
    "    \n",
    "    # Create updated rows for embedded sentences\n",
    "    df_updated_rows = df_meta_full.filter(\n",
    "        pl.col('sentenceID').is_in(embedded_ids)\n",
    "    ).with_columns([\n",
    "        pl.lit(model_info['embedding_id']).alias('embedding_id'),\n",
    "        pl.lit(model_info['model']).alias('embedding_model'),\n",
    "        ## pl.lit(model_info['dimensions']).alias('embedding_dims'),\n",
    "        pl.lit(model_info['dimensions']).cast(pl.Int16).alias('embedding_dims'),  # CAST to Int16\n",
    "        pl.lit(model_info['timestamp']).alias('embedding_date'),\n",
    "        pl.lit(vectors_s3_uri).alias('embedding_ref')  \n",
    "        ## // pl.lit(None).cast(pl.Utf8).alias('embedding_ref')\n",
    "    ])\n",
    "    \n",
    "    # Rows not embedded (keep unchanged)\n",
    "    df_unchanged_rows = df_meta_full.filter(\n",
    "        ~pl.col('sentenceID').is_in(embedded_ids)\n",
    "    )\n",
    "    \n",
    "    # ETL pattern: concat + unique\n",
    "    merged_meta = pl.concat([df_unchanged_rows, df_updated_rows])\n",
    "    merged_meta = merged_meta.unique(subset=['sentenceID'], keep='last')\n",
    "    \n",
    "    # Verify row count\n",
    "    assert len(merged_meta) == len(df_meta_full), \"Row count mismatch after meta update!\"\n",
    "    \n",
    "    total_embedded = merged_meta.filter(pl.col('embedding_id').is_not_null()).shape[0]\n",
    "    print(f\"  ✓ Updated: {total_embedded:,} total rows now have embeddings\")\n",
    "    \n",
    "    return merged_meta\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZATION & PROVIDER RESOLUTION\n",
    "# ============================================================================\n",
    "\n",
    "config = MLConfig()\n",
    "\n",
    "# Resolve paths once at start\n",
    "VECTORS_S3_KEY = config.embeddings_path(provider=None)\n",
    "META_S3_KEY = config.meta_embeds_path\n",
    "VECTORS_URI = f\"s3://{config.bucket}/{VECTORS_S3_KEY}\"\n",
    "META_URI = f\"s3://{config.bucket}/{META_S3_KEY}\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EMBEDDING GENERATION PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Mode: {config.embedding_mode}\")\n",
    "print(f\"Model: {config.bedrock_model_id} ({config.bedrock_dimensions}d)\")\n",
    "print(f\"\\n[Resolved Paths]\")\n",
    "print(f\"  Vectors: {VECTORS_S3_KEY}\")\n",
    "print(f\"  Meta: {META_S3_KEY}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: LOAD META TABLE\n",
    "# ============================================================================\n",
    "\n",
    "df_meta = load_meta_table_with_cache(config)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: FILTER SENTENCES\n",
    "# ============================================================================\n",
    "\n",
    "df_filtered, filtered_ids = filter_sentences(df_meta, config)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: GENERATE EMBEDDINGS\n",
    "# ============================================================================\n",
    "\n",
    "df_new_vectors, embedding_id, skipped_ids = generate_embeddings_batch(df_filtered, config)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: MERGE VECTORS TABLE\n",
    "# ============================================================================\n",
    "\n",
    "df_merged_vectors = merge_vectors_table(\n",
    "    new_vectors=df_new_vectors,\n",
    "    vectors_uri=VECTORS_URI,\n",
    "    storage_options=config.get_storage_options()\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: UPDATE META TABLE\n",
    "# ============================================================================\n",
    "\n",
    "model_info = {\n",
    "    'embedding_id': embedding_id,\n",
    "    'model': config.bedrock_model_id,\n",
    "    'dimensions': config.bedrock_dimensions,\n",
    "    'timestamp': datetime.now()\n",
    "}\n",
    "\n",
    "df_updated_meta = update_meta_table(\n",
    "    df_meta_full=df_meta,\n",
    "    filtered_ids=filtered_ids,\n",
    "    skipped_ids=skipped_ids,\n",
    "    model_info=model_info, \n",
    "    vectors_s3_uri=VECTORS_URI\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SAVE TO S3\n",
    "# S3 is source of truth, local cache for fast iteration\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: SAVE TO S3\n",
    "# S3 is source of truth, local cache for fast iteration\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n[Saving Results]\")\n",
    "\n",
    "# Derive filenames/folders from resolved S3 keys\n",
    "vectors_filename = Path(VECTORS_S3_KEY).name                      # e.g., finrag_embeddings_cohere_1024d.parquet\n",
    "vectors_provider = Path(VECTORS_S3_KEY).parent.name               # e.g., cohere_1024d\n",
    "meta_filename    = Path(META_S3_KEY).name                         # e.g., finrag_fact_sentences_meta_embeds.parquet\n",
    "\n",
    "# Canonical local cache locations (align with prep/cache cells)\n",
    "meta_cache_dir    = Path.cwd().parent / 'data_cache' / 'meta_embeds'\n",
    "vectors_cache_dir = Path.cwd().parent / 'data_cache' / 'embeddings' / vectors_provider\n",
    "\n",
    "meta_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "vectors_cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "meta_cache_path    = meta_cache_dir / meta_filename\n",
    "vectors_cache_path = vectors_cache_dir / vectors_filename\n",
    "\n",
    "# Save vectors to S3\n",
    "print(f\"  Vectors → S3: {VECTORS_URI}\")\n",
    "df_merged_vectors.write_parquet(\n",
    "    VECTORS_URI,\n",
    "    storage_options=config.get_storage_options(),\n",
    "    compression='zstd'\n",
    ")\n",
    "print(f\"  ✓ S3 saved: {len(df_merged_vectors):,} rows (Cost: $0.00 - ingress)\")\n",
    "\n",
    "# Save vectors to local cache (provider-scoped folder)\n",
    "print(f\"  Vectors → Local: {vectors_cache_path}\")\n",
    "df_merged_vectors.write_parquet(vectors_cache_path, compression='zstd')\n",
    "print(f\"  ✓ Cached locally\")\n",
    "\n",
    "# Save meta to S3\n",
    "print(f\"  Meta → S3: {META_URI}\")\n",
    "df_updated_meta.write_parquet(\n",
    "    META_URI,\n",
    "    storage_options=config.get_storage_options(),\n",
    "    compression='zstd'\n",
    ")\n",
    "print(f\"  ✓ S3 saved: {len(df_updated_meta):,} rows (Cost: $0.00 - ingress)\")\n",
    "\n",
    "# Save meta to local cache\n",
    "print(f\"  Meta → Local: {meta_cache_path}\")\n",
    "df_updated_meta.write_parquet(meta_cache_path, compression='zstd')\n",
    "print(f\"  ✓ Cached locally\")\n",
    "\n",
    "print(f\"\\n  Local cache locations:\")\n",
    "print(f\"    - {vectors_cache_path}\")\n",
    "print(f\"    - {meta_cache_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"✓ EMBEDDING PIPELINE COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  Mode: {config.embedding_mode}\")\n",
    "print(f\"  Sentences embedded: {len(df_new_vectors):,}\")\n",
    "if skipped_ids:\n",
    "    print(f\"  Sentences skipped: {len(skipped_ids):,}\")\n",
    "print(f\"  Total vectors in storage: {len(df_merged_vectors):,}\")\n",
    "print(f\"  Total meta rows with embeddings: {df_updated_meta.filter(pl.col('embedding_id').is_not_null()).shape[0]:,}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818ff818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d071538",
   "metadata": {},
   "source": [
    "### Order Preserve Proof: Important.\n",
    "```\n",
    "\n",
    "# Building batch (order maintained)\n",
    "current_batch = []  # List preserves insertion order\n",
    "for idx, row in enumerate(sentences_data):\n",
    "    current_batch.append({'id': sent_id, 'text': sent_text})\n",
    "    all_sentence_ids.append(sent_id)  # Same order as batch\n",
    "\n",
    "# API call\n",
    "texts = [item['text'] for item in current_batch]  # Preserves order\n",
    "batch_embeddings = _call_bedrock_api(...)  # Returns in same order\n",
    "\n",
    "# Collection\n",
    "all_embeddings.extend(batch_embeddings)  # Appends in order\n",
    "\n",
    "\n",
    "df_vectors = pl.DataFrame({\n",
    "    'sentenceID': all_sentence_ids,  # [id_0, id_1, id_2, ...]\n",
    "    'embedding': all_embeddings       # [emb_0, emb_1, emb_2, ...]\n",
    "})\n",
    "\n",
    "# Row 0: sentenceID[0] → embedding[0]\n",
    "# Row 1: sentenceID[1] → embedding[1]\n",
    "# Perfect 1-to-1 mapping\n",
    "\n",
    "```\n",
    "\n",
    "### True cost analysis:\n",
    "```\n",
    "1. Polars: Serialize 469,252 rows → 281MB Parquet file (in RAM)\n",
    "2. PyArrow: Write to temporary buffer\n",
    "3. Boto3: PUT request to S3\n",
    "4. S3: Receives 281MB upload\n",
    "5. S3: Atomically replaces old object\n",
    "6. Old version: Deleted (or moved to versioning if enabled)\n",
    "\n",
    "Network transfer: 281MB upload (ingress = $0.00)\n",
    "S3 operation: PutObject (free)\n",
    "Storage: 281MB × $0.023/GB/month = $0.006/month\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126ecd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654fda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705a90a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d72cab40",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FinRAG ML (Python 3.11)",
   "language": "python",
   "name": "finrag_ml_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
